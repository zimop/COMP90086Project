{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2f7398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dlg' 'eix' 'zqw' ... 'zwb' 'ojs' 'ijp']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython import display\n",
    "import csv\n",
    "\n",
    "# splitting test data and validation data\n",
    "all_data = np.loadtxt(\"COMP90086_2023_TLLdataset/train.csv\",\n",
    "                 delimiter=\",\", dtype=str)\n",
    "\n",
    "left_imgs = all_data[:,0][1:]\n",
    "right_imgs = all_data[:,1][1:]\n",
    "\n",
    "## path for training data\n",
    "data_dir = \"COMP90086_2023_TLLdataset/train\"\n",
    "\n",
    "##split data into 20% testing and 60% training and 20% validation\n",
    "left_train, left_test, right_train, right_test = train_test_split(left_imgs, right_imgs, random_state=104, test_size = 0.2, shuffle=True)\n",
    "left_train, left_val, right_train, right_val = train_test_split(left_train, right_train, random_state=104, test_size = 0.25, shuffle=True)\n",
    "print(right_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b4884f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##convert image names to gray scale image arrays:\n",
    "def convert_left(array, data_dir):\n",
    "    image_str = data_dir+\"/left/\"+array[0]+\".jpg\"\n",
    "    img_array = cv2.imread(image_str, cv2.IMREAD_GRAYSCALE)\n",
    "    ##img_array = cv2.resize(img_array, (40, 49))\n",
    "    total = np.array([img_array])\n",
    "    \n",
    "    for val in array[1:]:\n",
    "        image_str = data_dir+\"/left/\"+val+\".jpg\"\n",
    "        img_array = cv2.imread(image_str, cv2.IMREAD_GRAYSCALE)\n",
    "        ##img_array = cv2.resize(img_array, (40, 49))\n",
    "        total = np.append(total, np.array([img_array]), axis = 0)\n",
    "    return total\n",
    "\n",
    "def convert_right(array, data_dir):\n",
    "    image_str = data_dir+\"/right/\"+array[0]+\".jpg\"\n",
    "    img_array = cv2.imread(image_str, cv2.IMREAD_GRAYSCALE)\n",
    "    ##img_array = cv2.resize(img_array, (40, 49))\n",
    "    total = np.array([img_array])\n",
    "    \n",
    "    for val in array[1:]:\n",
    "        image_str = data_dir+\"/right/\"+val+\".jpg\"\n",
    "        img_array = cv2.imread(image_str, cv2.IMREAD_GRAYSCALE)\n",
    "        ##img_array = cv2.resize(img_array, (40, 49))\n",
    "        total = np.append(total, np.array([img_array]), axis = 0)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26fa583a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 245, 200)\n"
     ]
    }
   ],
   "source": [
    "left_train_img_arrays = convert_left(left_train, data_dir)\n",
    "left_val_img_arrays = convert_left(left_val, data_dir)\n",
    "\n",
    "right_train_img_arrays = convert_right(right_train, data_dir)\n",
    "right_val_img_arrays = convert_right(right_val, data_dir)\n",
    "print(left_val_img_arrays.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "939b421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (1200, 49000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize and reshape the data\n",
    "left_train_img_arrays = left_train_img_arrays.astype('float32') / 255.\n",
    "left_val_img_arrays = left_val_img_arrays.astype('float32') / 255.\n",
    "\n",
    "right_train_img_arrays = right_train_img_arrays.astype('float32') / 255.\n",
    "right_val_img_arrays = right_val_img_arrays.astype('float32') / 255.\n",
    "\n",
    "\n",
    "#x_left_train = np.reshape(left_train_img_arrays, (len(left_train_img_arrays), 32, 32, 1))\n",
    "# x_test = np.reshape(test_data, (len(test_data), 28, 28, 1))\n",
    "\n",
    "x_left_train = left_train_img_arrays.reshape((len(left_train_img_arrays), np.prod(left_train_img_arrays.shape[1:])))\n",
    "x_left_val = left_val_img_arrays.reshape((len(left_val_img_arrays), np.prod(left_val_img_arrays.shape[1:])))\n",
    "\n",
    "x_right_train = right_train_img_arrays.reshape((len(right_train_img_arrays), np.prod(right_train_img_arrays.shape[1:])))\n",
    "x_right_val = right_val_img_arrays.reshape((len(right_val_img_arrays), np.prod(right_val_img_arrays.shape[1:])))\n",
    "\n",
    "print(\"train_images shape:\", x_left_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1c3f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "5/5 [==============================] - 5s 543ms/step - loss: 0.6931 - val_loss: 0.6923\n",
      "Epoch 2/35\n",
      "5/5 [==============================] - 2s 396ms/step - loss: 0.6892 - val_loss: 0.6855\n",
      "Epoch 3/35\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.6804 - val_loss: 0.6790\n",
      "Epoch 4/35\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.6760 - val_loss: 0.6767\n",
      "Epoch 5/35\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.6745 - val_loss: 0.6757\n",
      "Epoch 6/35\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6731 - val_loss: 0.6738\n",
      "Epoch 7/35\n",
      "5/5 [==============================] - 2s 409ms/step - loss: 0.6714 - val_loss: 0.6742\n",
      "Epoch 8/35\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.6708 - val_loss: 0.6724\n",
      "Epoch 9/35\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.6700 - val_loss: 0.6722\n",
      "Epoch 10/35\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.6694 - val_loss: 0.6718\n",
      "Epoch 11/35\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.6690 - val_loss: 0.6715\n",
      "Epoch 12/35\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.6684 - val_loss: 0.6711\n",
      "Epoch 13/35\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6680 - val_loss: 0.6709\n",
      "Epoch 14/35\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.6676 - val_loss: 0.6713\n",
      "Epoch 15/35\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.6672 - val_loss: 0.6701\n",
      "Epoch 16/35\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6665 - val_loss: 0.6706\n",
      "Epoch 17/35\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.6663 - val_loss: 0.6694\n",
      "Epoch 18/35\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.6656 - val_loss: 0.6700\n",
      "Epoch 19/35\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.6653 - val_loss: 0.6687\n",
      "Epoch 20/35\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.6647 - val_loss: 0.6697\n",
      "Epoch 21/35\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.6646 - val_loss: 0.6685\n",
      "Epoch 22/35\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.6641 - val_loss: 0.6685\n",
      "Epoch 23/35\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.6641 - val_loss: 0.6692\n",
      "Epoch 24/35\n",
      "5/5 [==============================] - 2s 370ms/step - loss: 0.6634 - val_loss: 0.6683\n",
      "Epoch 25/35\n",
      "5/5 [==============================] - 2s 369ms/step - loss: 0.6632 - val_loss: 0.6688\n",
      "Epoch 26/35\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.6626 - val_loss: 0.6682\n",
      "Epoch 27/35\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6623 - val_loss: 0.6683\n",
      "Epoch 28/35\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.6621 - val_loss: 0.6688\n",
      "Epoch 29/35\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.6613 - val_loss: 0.6686\n",
      "Epoch 30/35\n",
      "5/5 [==============================] - 2s 366ms/step - loss: 0.6613 - val_loss: 0.6692\n",
      "Epoch 31/35\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.6632 - val_loss: 0.6692\n",
      "Epoch 32/35\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.6637 - val_loss: 0.6732\n",
      "Epoch 33/35\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.6625 - val_loss: 0.6694\n",
      "Epoch 34/35\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6605 - val_loss: 0.6692\n",
      "Epoch 35/35\n",
      "5/5 [==============================] - 2s 367ms/step - loss: 0.6598 - val_loss: 0.6694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe350e9a100>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the size of our encoded representations\n",
    "latent_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(49000,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(latent_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(49000, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)\n",
    "\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_left_train, x_right_train,\n",
    "                epochs=35,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_left_val, x_right_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7c1dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9e01be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "%s : %d osr 39.406254\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "%s : %d xjj 23.985691\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "%s : %d lyf 55.649857\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "%s : %d mqw 17.280386\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "%s : %d dlg 5.51256\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "%s : %d eix 9.165424\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "%s : %d zqw 1.4398232\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "%s : %d zwb 3.2566228\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "%s : %d ojs 13.368179\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "%s : %d ijp 17.384405\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load and preprocess two images\n",
    "img_path1 = np.array(['aif'])\n",
    "img1 = convert_left(img_path1, data_dir)\n",
    "left_img = img1.astype('float32') / 255.\n",
    "x_left = left_img.reshape((len(left_img), np.prod(left_img.shape[1:])))\n",
    "encoded_features_left = encoder.predict(x_left)\n",
    "\n",
    "right_imgs = np.array(['osr','xjj', 'lyf' , 'mqw', 'dlg' ,'eix' ,'zqw', 'zwb' ,'ojs' ,'ijp'])\n",
    "\n",
    "for val in right_imgs:\n",
    "    img_path2 = np.array([val])\n",
    "    img2 = convert_right(img_path2, data_dir)\n",
    "    right_img = img2.astype('float32') / 255.\n",
    "    x_right = right_img.reshape((len(right_img), np.prod(right_img.shape[1:])))\n",
    "\n",
    "    # use the encoder to get the feature vectors\n",
    "    encoded_features_left = encoder.predict(x_left)\n",
    "    encoded_features_right = encoder.predict(x_right)\n",
    "\n",
    "    # Calculate Euclidean distance\n",
    "    euclidean_distance = np.linalg.norm(encoded_features_left - encoded_features_right)\n",
    "    \n",
    "    print(\"%s : %d\", val, euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ae473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d4b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f99a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
